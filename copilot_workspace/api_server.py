"""\nTH_e_ART_h_MAN - AI Copilot Workspace API Server\nAdvanced REST API for Stable Diffusion Prompt Generation\n"""\n\nimport sys\nimport os\nimport json\nfrom flask import Flask, request, jsonify, send_from_directory\nfrom flask_cors import CORS\nimport logging\n\n# Add tools directory to path for OllamaBridge import\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', 'tools'))\n\ntry:\n    from ollama_bridge import OllamaBridge\n    OLLAMA_AVAILABLE = True\nexcept ImportError:\n    OLLAMA_AVAILABLE = False\n    print(\"OllamaBridge not available - running in demo mode\")\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\napp = Flask(__name__)\nCORS(app)\n\nclass AICopilotAPI:\n    def __init__(self):\n        self.bridge = None\n        self.demo_mode = True\n        \n        if OLLAMA_AVAILABLE:\n            try:\n                self.bridge = OllamaBridge()\n                if self.bridge.is_available():\n                    self.demo_mode = False\n                    logger.info(\"OllamaBridge connected successfully\")\n                else:\n                    logger.warning(\"Ollama not available - using demo mode\")\n            except Exception as e:\n                logger.error(f\"OllamaBridge initialization failed: {e}\")\n        \n        logger.info(f\"AI Copilot API initialized - Demo Mode: {self.demo_mode}\")\n\n    def get_ai_response(self, message: str, model: str = \"main\") -> str:\n        \"\"\"Get AI response for chat message.\"\"\"\n        if not self.demo_mode and self.bridge:\n            try:\n                return self.bridge.chat(message, model)\n            except Exception as e:\n                logger.error(f\"Bridge error: {e}\")\n                return self.get_fallback_response(message)\n        else:\n            return self.get_fallback_response(message)\n\n    def enhance_prompt(self, basic_prompt: str, style: str, quality: str) -> str:\n        \"\"\"Enhance a basic prompt with style and quality modifiers.\"\"\"\n        if not self.demo_mode and self.bridge:\n            try:\n                enhancement_message = f\"\"\"Enhance this Stable Diffusion prompt:\nBasic: {basic_prompt}\nStyle: {style}\nQuality: {quality}\n\nProvide only the enhanced prompt without explanation.\"\"\"\n                return self.bridge.chat(enhancement_message, \"quick\")\n            except Exception as e:\n                logger.error(f\"Enhancement error: {e}\")\n                return self.get_enhanced_fallback(basic_prompt, style, quality)\n        else:\n            return self.get_enhanced_fallback(basic_prompt, style, quality)\n\n    def get_fallback_response(self, message: str) -> str:\n        \"\"\"Provide fallback responses when Ollama is not available.\"\"\"\n        fallback_responses = {\n            'sunset': 'Breathtaking sunset over mountains, golden hour lighting, dramatic clouds, warm color palette, landscape photography, high detail, cinematic composition',\n            'portrait': 'Professional portrait, studio lighting, sharp focus, detailed facial features, photorealistic, high quality, 85mm lens, bokeh background',\n            'fantasy': 'Fantasy landscape, magical atmosphere, ethereal lighting, mystical creatures, detailed environment, concept art style, high fantasy',\n            'anime': 'Anime character, detailed illustration, vibrant colors, manga style, high quality artwork, digital art',\n            'digital': 'Digital art, modern aesthetic, clean composition, vibrant colors, detailed illustration, contemporary style'\n        }\n        \n        # Simple keyword matching\n        message_lower = message.lower()\n        for keyword, response in fallback_responses.items():\n            if keyword in message_lower:\n                return f\"Based on your request: '{message}'\\n\\nEnhanced Prompt Generated:\\n{response}\"\n        \n        # Generic fallback\n        return f\"Simulated AI Response (Demo Mode)\\n\\nFor your request: '{message}'\\n\\nI would help you generate detailed Stable Diffusion prompts with artistic suggestions and technical parameters.\"\n\n    def get_enhanced_fallback(self, basic_prompt: str, style: str, quality: str) -> str:\n        \"\"\"Provide fallback prompt enhancement.\"\"\"\n        style_enhancements = {\n            'photo': 'photorealistic, high resolution, detailed, professional photography',\n            'digital': 'digital art, detailed, vibrant colors, modern aesthetic',\n            'oil': 'oil painting, classical art style, rich textures, painterly',\n            'anime': 'anime style, detailed, vibrant colors, manga aesthetic',\n            'water': 'watercolor painting, soft colors, artistic, flowing',\n            'sketch': 'pencil sketch, detailed linework, artistic, monochrome'\n        }\n        \n        quality_enhancements = {\n            'standard': 'good quality, clean composition',\n            'high': 'high quality, sharp focus, detailed, professional',\n            'ultra': 'ultra high quality, 8k resolution, masterpiece, highly detailed',\n            'artistic': 'artistic masterpiece, creative composition, unique style'\n        }\n        \n        style_text = style_enhancements.get(style, 'artistic style')\n        quality_text = quality_enhancements.get(quality, 'good quality')\n        \n        return f\"{basic_prompt}, {style_text}, {quality_text}\"\n\n# Initialize API\napi = AICopilotAPI()\n\n# Routes\n@app.route('/')\ndef serve_index():\n    \"\"\"Serve the main HTML interface.\"\"\"\n    return send_from_directory('.', 'index.html')\n\n@app.route('/<path:filename>')\ndef serve_static(filename):\n    \"\"\"Serve static files.\"\"\"\n    return send_from_directory('.', filename)\n\n@app.route('/api/status')\ndef get_status():\n    \"\"\"Get system status.\"\"\"\n    return jsonify({\n        'success': True,\n        'ollama_connected': not api.demo_mode,\n        'demo_mode': api.demo_mode,\n        'available_models': ['main', 'quick', 'code'] if api.bridge else ['demo']\n    })\n\n@app.route('/api/chat', methods=['POST'])\ndef chat():\n    \"\"\"Handle chat messages.\"\"\"\n    try:\n        data = request.get_json()\n        message = data.get('message', '')\n        model = data.get('model', 'main')\n        \n        if not message:\n            return jsonify({'success': False, 'error': 'No message provided'})\n        \n        response = api.get_ai_response(message, model)\n        \n        return jsonify({\n            'success': True,\n            'response': response,\n            'model': model,\n            'demo_mode': api.demo_mode\n        })\n        \n    except Exception as e:\n        logger.error(f\"Chat error: {e}\")\n        return jsonify({'success': False, 'error': str(e)})\n\n@app.route('/api/enhance', methods=['POST'])\ndef enhance():\n    \"\"\"Handle prompt enhancement requests.\"\"\"\n    try:\n        data = request.get_json()\n        prompt = data.get('prompt', '')\n        style = data.get('style', 'photo')\n        quality = data.get('quality', 'standard')\n        \n        if not prompt:\n            return jsonify({'success': False, 'error': 'No prompt provided'})\n        \n        enhanced_prompt = api.enhance_prompt(prompt, style, quality)\n        \n        return jsonify({\n            'success': True,\n            'enhanced_prompt': enhanced_prompt,\n            'original_prompt': prompt,\n            'style': style,\n            'quality': quality,\n            'demo_mode': api.demo_mode\n        })\n        \n    except Exception as e:\n        logger.error(f\"Enhancement error: {e}\")\n        return jsonify({'success': False, 'error': str(e)})\n\n@app.route('/api/models')\ndef get_models():\n    \"\"\"Get available models.\"\"\"\n    if api.bridge and not api.demo_mode:\n        models = api.bridge.get_available_models()\n        return jsonify({'success': True, 'models': models})\n    else:\n        return jsonify({\n            'success': True,\n            'models': {'main': 'Demo Main', 'quick': 'Demo Quick', 'code': 'Demo Code'},\n            'demo_mode': True\n        })\n\nif __name__ == '__main__':\n    print(\"Starting TH_e_ART_h_MAN API Server...\")\n    print(f\"Demo Mode: {api.demo_mode}\")\n    print(\"Access the interface at: http://localhost:5000\")\n    \n    app.run(\n        host='0.0.0.0',\n        port=5000,\n        debug=True\n    )