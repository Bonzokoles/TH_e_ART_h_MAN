"""\nAI Copilot Chat Interface - Gradio-based conversational prompt generation\n\nProvides a modern, user-friendly interface for generating Stable Diffusion\nprompts through natural conversation with local Ollama models.\n"""\n\nimport sys\nimport os\nfrom typing import List, Tuple, Optional\nimport gradio as gr\nfrom rich.console import Console\nimport argparse\n\n# Add tools directory to path\nsys.path.append(os.path.join(os.path.dirname(__file__), '..', 'tools'))\n\ntry:\n    from ollama_bridge import OllamaBridge\nexcept ImportError as e:\n    print(f\"Warning: Could not import OllamaBridge: {e}\")\n    OllamaBridge = None\n\ntry:\n    from wildcard_generator import WildcardGenerator\nexcept ImportError as e:\n    print(f\"Warning: Could not import WildcardGenerator: {e}\")\n    WildcardGenerator = None\n\nconsole = Console()\n\nclass ChatInterface:\n    """\n    Gradio-based chat interface for AI Copilot Workspace.\n    \n    Features:\n    - Natural language conversation for prompt generation\n    - Model selection (main, quick, code)\n    - Wildcard-based prompt enhancement\n    - Export functionality\n    """\n    \n    def __init__(self):\n        """Initialize chat interface with OllamaBridge."""\n        self.bridge = None\n        self.current_model = \"main\"\n        \n        # Initialize bridge if available\n        if OllamaBridge:\n            try:\n                self.bridge = OllamaBridge()\n                console.print(\"OllamaBridge initialized\", style=\"green\")\n            except Exception as e:\n                console.print(f\"Bridge init failed: {e}\", style=\"yellow\")\n        \n        # Initialize wildcard generator\n        self.wildcard_gen = None\n        if WildcardGenerator:\n            try:\n                self.wildcard_gen = WildcardGenerator()\n                console.print(\"WildcardGenerator initialized\", style=\"green\")\n            except Exception as e:\n                console.print(f\"WildcardGenerator init failed: {e}\", style=\"yellow\")\n    \n    def chat_with_ai(self, message: str, history: List[Tuple[str, str]], \n                     model_choice: str) -> Tuple[List[Tuple[str, str]], str]:\n        \"\"\"Process user message and generate AI response.\"\"\"\n        if not message.strip():\n            return history, \"\"\n        \n        self.current_model = model_choice\n        \n        # Handle special commands\n        if message.lower().startswith('/'):\n            response = self._handle_command(message)\n        else:\n            response = self._generate_response(message, model_choice)\n        \n        # Update history\n        history.append((message, response))\n        return history, \"\"\n    \n    def _generate_response(self, message: str, model: str) -> str:\n        \"\"\"Generate AI response using OllamaBridge.\"\"\"\n        if not self.bridge:\n            return self._fallback_response(message)\n        \n        try:\n            if not self.bridge.is_available():\n                return (\"Ollama service is not running. Please start Ollama with: `ollama serve`\\n\\n\"\n                       f\"Simulated response for: {message}\")\n            \n            enhanced_message = self._enhance_prompt(message, model)\n            response = self.bridge.chat(enhanced_message, model=model)\n            \n            return response if response else \"No response generated. Please try again.\"\n            \n        except Exception as e:\n            console.print(f\"Chat error: {e}\", style=\"red\")\n            return f\"Error: {str(e)}. Please check Ollama connection.\"\n    \n    def _enhance_prompt(self, message: str, model: str) -> str:\n        \"\"\"Enhance user message with context based on model type.\"\"\"\n        context_prompts = {\n            \"main\": \"You are an expert AI assistant for Stable Diffusion prompt generation. Help users create detailed, artistic prompts.\",\n            \"quick\": \"Generate concise, effective Stable Diffusion prompts. Focus on essential keywords and artistic style.\",\n            \"code\": \"You assist with structured prompt generation and technical aspects of Stable Diffusion.\"\n        }\n        \n        context = context_prompts.get(model, context_prompts[\"main\"])\n        return f\"{context}\\n\\nUser request: {message}\"\n    \n    def _fallback_response(self, message: str) -> str:\n        \"\"\"Provide fallback response when OllamaBridge is not available.\"\"\"\n        return (f\"Simulated Response (Ollama not connected)\\n\\n\"\n               f\"For your request: {message}\\n\\n\"\n               \"I would help you generate detailed Stable Diffusion prompts \"\n               \"with artistic suggestions and technical parameters.\")\n    \n    def _handle_command(self, command: str) -> str:\n        \"\"\"Handle special chat commands.\"\"\"\n        cmd = command.lower().strip()\n        \n        if cmd == '/help':\n            return (\"Available Commands:\\n\"\n                   \"â€¢ /help - Show this help\\n\"\n                   \"â€¢ /clear - Clear conversation history\\n\"\n                   \"â€¢ /models - Show available models\\n\"\n                   \"â€¢ /status - Check system status\")\n        \n        elif cmd == '/clear':\n            if self.bridge:\n                self.bridge.clear_history()\n            return \"Conversation cleared!\"\n        \n        elif cmd == '/models':\n            if self.bridge:\n                models = self.bridge.get_available_models()\n                return f\"Available Models: {list(models.keys())}\"\n            return \"Available Models: main, quick, code\"\n        \n        elif cmd == '/status':\n            if self.bridge:\n                status = self.bridge.get_status()\n                return f\"System Status: Ollama {'Online' if status['available'] else 'Offline'}\"\n            return \"System Status: Bridge not available\"\n        \n        return f\"Unknown command: {command}\"\n    \n    def generate_wildcards(self, template: str, count: int = 5) -> str:\n        \"\"\"Generate wildcard variations.\"\"\"\n        if not self.wildcard_gen:\n            return \"WildcardGenerator not available\"\n        \n        try:\n            variations = self.wildcard_gen.generate_variations(template, count)\n            return \"\\n\".join([f\"{i+1}. {var}\" for i, var in enumerate(variations)])\n        except Exception as e:\n            return f\"Error generating wildcards: {e}\"\n    \n    def create_interface(self) -> gr.Blocks:\n        \"\"\"Create and return Gradio interface.\"\"\"\n        with gr.Blocks(title=\"TH_e_ART_h_MAN - AI Copilot\", theme=gr.themes.Soft()) as interface:\n            \n            gr.Markdown(\"# ðŸ§™ TH_e_ART_h_MAN - AI Copilot Workspace\")\n            gr.Markdown(\"*AI-powered Stable Diffusion prompt generation*\")\n            \n            with gr.Tab(\"ðŸ’¬ Chat AI\"):\n                with gr.Row():\n                    with gr.Column(scale=3):\n                        chatbot = gr.Chatbot(\n                            label=\"AI Conversation\",\n                            height=400,\n                            type=\"tuples\"  # Note: deprecated, should be 'messages'\n                        )\n                        \n                        with gr.Row():\n                            msg = gr.Textbox(\n                                label=\"Your Message\",\n                                placeholder=\"Ask me to generate or enhance a Stable Diffusion prompt...\",\n                                scale=4\n                            )\n                            send = gr.Button(\"Send\", variant=\"primary\", scale=1)\n                    \n                    with gr.Column(scale=1):\n                        model_choice = gr.Radio(\n                            choices=[\"main\", \"quick\", \"code\"],\n                            value=\"main\",\n                            label=\"Model\"\n                        )\n                        clear = gr.Button(\"Clear Chat\")\n            \n            with gr.Tab(\"ðŸŽ² Wildcard Generator\"):\n                with gr.Row():\n                    with gr.Column():\n                        template_input = gr.Textbox(\n                            label=\"Template\",\n                            placeholder=\"{characters} in {environments}, {art_styles} style\",\n                            lines=3\n                        )\n                        count_slider = gr.Slider(\n                            minimum=1, maximum=20, value=5,\n                            label=\"Number of Variations\"\n                        )\n                        generate_btn = gr.Button(\"Generate Variations\", variant=\"primary\")\n                    \n                    with gr.Column():\n                        variations_output = gr.Textbox(\n                            label=\"Generated Variations\",\n                            lines=12,\n                            interactive=False\n                        )\n            \n            # Function definitions for interface\n            def submit_message(message, history, model):\n                return self.chat_with_ai(message, history, model)\n            \n            def clear_conversation():\n                if self.bridge:\n                    self.bridge.clear_history()\n                return []\n            \n            def generate_variations(template, count):\n                return self.generate_wildcards(template, int(count))\n            \n            # Event handlers\n            send.click(\n                submit_message,\n                inputs=[msg, chatbot, model_choice],\n                outputs=[chatbot, msg]\n            )\n            \n            msg.submit(\n                submit_message,\n                inputs=[msg, chatbot, model_choice],\n                outputs=[chatbot, msg]\n            )\n            \n            clear.click(\n                clear_conversation,\n                outputs=[chatbot]\n            )\n            \n            generate_btn.click(\n                generate_variations,\n                inputs=[template_input, count_slider],\n                outputs=[variations_output]\n            )\n            \n            # Welcome message\n            interface.load(\n                lambda: [(\"Welcome! ðŸ‘‹\", \"Hi! I'm your AI Copilot for Stable Diffusion. Ask me to generate prompts or use the Wildcard Generator tab!\")],\n                outputs=[chatbot]\n            )\n        \n        return interface\n\ndef launch_chat_interface(share: bool = False, port: int = 7860):\n    \"\"\"Launch the Gradio chat interface.\"\"\"\n    console.print(\"Launching AI Copilot Chat Interface...\", style=\"blue\")\n    \n    # Create interface\n    chat = ChatInterface()\n    interface = chat.create_interface()\n    \n    # Launch\n    try:\n        interface.launch(\n            share=share,\n            server_port=port,\n            show_error=True,\n            show_tips=True\n        )\n    except Exception as e:\n        console.print(f\"Failed to launch interface: {e}\", style=\"red\")\n        return False\n    \n    return True\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"TH_e_ART_h_MAN - AI Copilot Chat Interface\")\n    parser.add_argument(\"--share\", action=\"store_true\", help=\"Create public Gradio link\")\n    parser.add_argument(\"--port\", type=int, default=7860, help=\"Port for local server\")\n    \n    args = parser.parse_args()\n    launch_chat_interface(share=args.share, port=args.port)