"""\nOllamaBridge - Core connection layer between workspace and Ollama models\n\nProvides seamless integration with local Ollama instance for AI Copilot\nWorkspace. Supports multiple models with intelligent selection and\nconversation context management.\n"""\n\nimport requests\nimport time\nfrom typing import Dict, List\nfrom rich.console import Console\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass OllamaBridge:\n    """\n    Bridge class for connecting to local Ollama models.\n    \n    Features:\n    - Multiple model support with smart selection\n    - Conversation context management\n    - Streaming responses for real-time interaction\n    - Error handling and reconnection logic\n    - Memory-efficient model switching\n    """\n    \n    def __init__(self, base_url: str = "http://localhost:11434"):\n        """\n        Initialize OllamaBridge with connection to Ollama service.\n        \n        Args:\n            base_url (str): Ollama service URL\n        """\n        self.base_url = base_url.rstrip('/')\n        self.console = Console()\n        \n        # Model definitions with use cases\n        self.models = {\n            "main": "wizard-uncensored:7b",    # Complex conversations\n            "quick": "gemma3:1b",              # Fast prompt generation\n            "code": "llama3.2:3b"              # Structured outputs\n        }\n        \n        # Conversation context storage\n        self.conversation_history: List[Dict[str, str]] = []\n        self.max_history_length = 10  # Keep last 10 exchanges\n        \n        # Connection state\n        self._last_check = 0\n        self._check_interval = 30  # Check connection every 30 seconds\n        self._is_available = None\n        \n        logger.info(f"OllamaBridge initialized with base URL: {self.base_url}")\n        \n    def is_available(self) -> bool:\n        """\n        Check if Ollama service is available and responsive.\n        \n        Returns:\n            bool: True if service is available, False otherwise\n        """\n        current_time = time.time()\n        \n        # Use cached result if within check interval\n        if (self._is_available is not None and \n            current_time - self._last_check < self._check_interval):\n            return self._is_available\n            \n        try:\n            response = requests.get(f"{self.base_url}/api/tags", timeout=5)\n            self._is_available = response.status_code == 200\n            self._last_check = current_time\n            \n            if self._is_available:\n                logger.info("Ollama service is available")\n            else:\n                logger.warning(f"Ollama responded with status: {response.status_code}")\n                \n        except requests.exceptions.RequestException as e:\n            self._is_available = False\n            self._last_check = current_time\n            logger.error(f"Cannot connect to Ollama: {e}")\n            \n        return self._is_available\n\n    def chat(self, message: str, model: str = "main", add_to_history: bool = True) -> str:\n        """\n        Send a message to specified Ollama model and get response.\n        \n        Args:\n            message (str): User message to send\n            model (str): Model to use ('main', 'quick', 'code')\n            add_to_history (bool): Whether to add to conversation history\n            \n        Returns:\n            str: Model response\n        """\n        if not self.is_available():\n            raise ConnectionError("Ollama service is not available")\n            \n        if model not in self.models:\n            raise ValueError(f"Invalid model '{model}'. Available: {list(self.models.keys())}")\n        \n        model_name = self.models[model]\n        context = self._build_context(message, add_to_history)\n        \n        try:\n            response = requests.post(\n                f"{self.base_url}/api/generate",\n                json={\n                    "model": model_name,\n                    "prompt": context,\n                    "stream": False,\n                    "options": {\n                        "temperature": 0.7,\n                        "top_k": 40,\n                        "top_p": 0.9\n                    }\n                },\n                timeout=30\n            )\n            \n            response.raise_for_status()\n            result = response.json()\n            ai_response = result.get("response", "").strip()\n            \n            if add_to_history and ai_response:\n                self._add_to_history("user", message)\n                self._add_to_history("assistant", ai_response)\n            \n            return ai_response\n            \n        except requests.exceptions.RequestException as e:\n            logger.error(f"Request failed: {e}")\n            raise ConnectionError(f"Failed to communicate with Ollama: {e}")\n\n    def _build_context(self, message: str, include_history: bool) -> str:\n        """Build conversation context with history."""\n        if not include_history or not self.conversation_history:\n            return message\n            \n        context_parts = []\n        for entry in self.conversation_history[-self.max_history_length:]:\n            role = entry["role"]\n            content = entry["content"]\n            \n            if role == "user":\n                context_parts.append(f"Human: {content}")\n            elif role == "assistant":\n                context_parts.append(f"Assistant: {content}")\n        \n        context_parts.append(f"Human: {message}")\n        context_parts.append("Assistant:")\n        \n        return "\\n".join(context_parts)\n\n    def _add_to_history(self, role: str, content: str):\n        """Add entry to conversation history with automatic cleanup."""\n        self.conversation_history.append({\n            "role": role,\n            "content": content,\n            "timestamp": time.time()\n        })\n        \n        if len(self.conversation_history) > self.max_history_length * 2:\n            self.conversation_history = self.conversation_history[-self.max_history_length:]\n\n    def clear_history(self):\n        """Clear conversation history."""\n        self.conversation_history.clear()\n        logger.info("Conversation history cleared")\n\n    def get_available_models(self) -> Dict[str, str]:\n        """Get list of configured models."""\n        return self.models.copy()\n\n    def get_status(self) -> Dict[str, any]:\n        """Get current bridge status and statistics."""\n        return {\n            "available": self.is_available(),\n            "base_url": self.base_url,\n            "models": self.models,\n            "conversation_length": len(self.conversation_history)\n        }